\chapter{Grundlagen}
Für das Verständnis der Ausarbeitung werden einige Grundlagen benötigt.
In diesem Kapitel werden diese Grundlagen übermittelt.
Es werden die wichtigsten Eigenschaften des Serverless-Computing erläutert,
sowie gängige serverlose Dienste vorgestellt.

\section{Serverless-Computing}
Serverless-Computing ist ein Konzept, mit dem man skalierbare Anwendungen
entwickeln und ohne komplexe Serververwaltung bereitstellen kann \cite{CioGov}.
Das Konzept ist aktuell bei den öffentlichen Cloud-Providern beliebt, die viele
serverlose Dienste für Entwickler und Unternehmen bereitstellen. Trotz des Namens
werden selbstverständlich immer noch \textit{Server} verwendet. Der Name symbolisiert die geringe
und einfache Serververwaltung im Vergleich zur den üblichen selbstverwalteten
Diensten auf \cite{CNCF}.
Häufig wird der Begriff \textit{serverless} mit einer ganz bestimmten serverlosen Dienstart
assoziiert, den \textit{Function-as-a-Service} (FaaS).

Die Idee ist, dass Entwickler ihre Anwendungslogik aus isolierten und 
zustandslosen Funktionen aufbauen. Es ist vorgesehen, dass jede Funktion nur für eine
ganz spezifische Aufgabe zuständig ist. Die Funktionen können
durch Ereignisse (engl. \textit{Events}) ausgelöst werden. Treffen solche Ereignisse ein,
provisioniert der Cloudanbieter die passenden Ressourcen und führt die jeweilige Funktion aus.
Aufgrund des geringen Aufgabenbereichs und Zustandslosigkeit einer Funktion,
sind Cloudanbieter in der Lage sehr schnell mehrere Instanzen einer Funktion zu starten,
um eine erhöhte Nachfrage zu bewältigen.
Verringert sich die Nachfrage wieder, werden die Instanzen abgebaut. Anders ist 
es bei traditionellen selbstverwalteten Diensten. Dort müssen Entwickler die Nachfrage grob
abschätzen und bei zu hoher Nachfrage die Kapazitäten rechtzeitig
erhöhen \cite{WhatIsServerless} \cite{ServerlessTrends}.

Ein weiterer Vorteil des Serverless-Computing ist, dass Cloudanbieter
bei ihren serverlosen Diensten nur die eigentlichen Rechentätigkeiten
in Zahlung stellen, sodass für Entwickler und Unternehmen nur dann Kosten
anfallen, wenn ihre Anwendungen auch benutzt werden \cite{EcoArc}.

Auch das Wegfallen der Provisionierung und täglichen Verwaltung bringt Vorteile mit sich.
Entwickler können sich auf die eigentliche Entwicklung der Anwendungen konzentrieren und
müssen sich nicht mit den täglichen Kleinigkeiten des Entwicklungsalltags
beschäftigen \cite{ServerlessTrends}.

Somit lässt sich das Serverless-Computing Konzept gut auf Applikationen anwenden,
die über den Tag hinaus eine starke Schwankung in der Nachfrage haben. Der Cloudanbieter wird
die angefragten Funktionen passend hoch und runter skalieren. Auch wenn bereits stark auf 
andere Services eines Cloud-Providers zurückgegriffen wird, sind die serverlosen Dienste
gut für das Verknüpfen der Services geeignet. Da die einzelnen Funktionen durch Events
aktiviert werden, können Daten in eine Funktion eingeführt werden, um
dann an die benötigten Komponenten, in passendem Dateiformat, wieder ausgeführt zu werden
\cite{ServerlessTrends} \cite{HpcServerless}.

Um in den folgenden Kapiteln der Arbeit tiefer in die Architektur einer serverlosen Platform
eintauchen zu können, müssen die zwei Begriffe Virtual-Machine und Container erklärt werden.

\section{Virtual-Machine}
Bei einer Virtual-Machine handelt es sich um eine virtuelle Umgebung, mit komplett 
eigenen Ressourcen, wie unter anderem CPU und Speicher.
Möglich wird die Virtualisierung durch den \textit{Virtual Machine Monitor} (VMM),
häufig auch \textit{Hypervisor} genannt.
Der Hypervisor ist ein Softwarestück, das dafür zuständig ist die physischen
und virtuellen Ressourcen zu verwalten. Er ist für die Kommunikation
zwischen den beiden zuständig, denn die Arbeit in einer
virtuellen Umgebung wird immer noch von den physisch vorhanden Hardwareressourcen ausgeführt.
Hier übernimmt der Hypervisor die Planung (engl. \textit{Scheduling}) und das Abbilden
der Anfragen und Instruktionen aus den virtuellen Umgebungen auf die Hardwareressourcen.
Ein großer Vorteil der Virtualisierung von Hardwareressourcen ist,
dass jede Virtual-Machine ihr eigenes Betriebssystem benutzen kann
\cite{RedHatVM} \cite{RedHatHypervisor}.

Es gibt zwei unterschiedliche Arten von Hypervisoren. Im nachfolgenden Verlauf der Arbeit
ist die zweite Art zu vernachlässigen, wird aufgrund der Vollständigkeit hier erwähnt.

\subsection{Native Hypervisor}
Ein nativer Hypervisor, auch Typ 1 Hypervisor genannt, nimmt den Platz des eigentlichen Betriebssystems ein
und verwaltet die virtuellen Umgebungen und ihre Betriebssysteme. Er läuft direkt
auf der Hardware und dirigiert die virtuellen Ressourcen zu den physischen Ressourcen.
Bekannte Typ 1 Hypervisor sind unter anderem
\href{https://docs.microsoft.com/de-de/virtualization/hyper-v-on-windows/}{Microsoft Hyper-V},
\href{https://www.vmware.com/de/products/vsphere.html}{VMware vSphere} und
\href{https://www.linux-kvm.org/page/Main_Page}{KVM}, welcher in den Linux
Kernel eingefügt wurde \cite{RedHatHypervisor}.

\subsection{Hosted Hypervisor}
Ein hosted Hypervisor, auch Typ 2 Hypervisor genannt, läuft als übliche
Applikation auf dem Host-Betriebssystem.
Ein Beispiel hierfür wäre VirtualBox von Oracle \cite{VirtualBox}.
Die virtuellen Ressourcen kommunizieren mit dem Host-Betriebssystem,
welcher wiederum die Instruktionen an die physischen Hardwareressourcen weiterleitet.
Somit laufen die virtuellen Umgebungen auf dem Host-Betriebssystem \cite{RedHatHypervisor}.

\section{Container}
Ein Linux Container beinhalten fast vollständig isolierte Prozesse.
Durch eine Konfigurationsdatei (\textit{Image}) werden alle nötigen Ressourcen und Dateien,
die für das Ausführen der Prozesse benötigt werden, definiert. Ähnlich wie ein Typ 2 Hypervisor
laufen Container auf dem Host-Betriebssystem, mit dem großen Unterschied, dass die einzelnen
Container selbst keine eigenen Betriebssysteme haben. Container teilen sich das
Host-Betriebssystem und \textit{Kernel}.
Dadurch haben Container den Vorteil, dass sie kompakter und viel sparsamer sind.

1. Was sind Container?
-- Verpacken und isolieren von Software in einer produktionsreifen Laufzeitumgebung\\
-- Erleichtert das Entwickeln auf unterschiedlichen Systemen und Umgebungen\\
-- Keine vollständige Isolation\\
-- Host und Container können sich Ressourcen teilen\\
-- Relative ressourcenarm\\

\section{Function as a Service}
-- Ein Cloud Computing Service wie PaaS, IaaS, SaaS\\
-- Kleine, stateless Funktionen die einzelnd abgerechnet und skaliert werden\\

\section{Serverless Containers}
-- Laufzeitumgebung der serverlosen Anwendung wird durch Dockerimage bereitgestellt
-- Anwendung kann beliebigen aufbau haben

\section{Open-Source und self-hosted Lösungen}
-- OpenFaaS
-- IBM OpenWhisk
-- Oracle Fn Project

\section{Sichere Umgebungen}
-- VMs sind zu Ressourcen intensiv
-- Container sind nicht sicher

\subsection{Google gVisor}
-- Applikationskernel für Container in Golang\\
-- Implementiert viele des Linux kernel interfaces\\
-- Fängt SystemCalls ab und gibt sich als GuestKernel aus\\
% TODO: siehe gVisor Doku und Whitepapers\\

\subsection{AWS Firecracker}
-- Benutzt KVM (Kernel-based Virtaul Machine, ähnlich sVMs)\\
% TODO: https://assets.amazon.science/96/c6/302e527240a3b1f86c86c3e8fc3d/firecracker-lightweight-virtualization-for-serverless-applications.pdf

\section{Eigenschaften serverloser Architekturen}
-- Architekturdartstellung einer simplen Webanwendung, Frontend + Backend + Datenbank
-- Vier eigenschaften des Serverless Computing bei jedem der Services deutlich machen
-- Serverless sieht vor soviel wie möglich die Dienste der Cloud-Provider zuverwenden