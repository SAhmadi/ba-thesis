\chapter{Grundlagen}
Für das Verständnis der Ausarbeitung werden einige Grundlagen benötigt.
In diesem Kapitel werden diese Grundlagen übermittelt.
Es werden die wichtigsten Eigenschaften des Serverless-Computing erläutert,
sowie gängige serverlose Dienste vorgestellt.

\section{Serverless-Computing}
Serverless-Computing ist ein Konzept, mit dem man skalierbare Anwendungen
entwickeln und ohne komplexe Serververwaltung bereitstellen kann \cite{CioGov}.
Das Konzept ist aktuell bei den öffentlichen Cloud-Providern beliebt, die viele
serverlose Dienste für Entwickler und Unternehmen bereitstellen. Trotz des Namens
werden selbstverständlich immer noch \textit{Server} verwendet. Der Name symbolisiert die geringe
und einfache Serververwaltung im Vergleich zur den üblichen selbstverwalteten
Diensten auf \cite{CNCF}.
Häufig wird der Begriff \textit{serverless} mit einer ganz bestimmten serverlosen Dienstart
assoziiert, den \textit{Function-as-a-Service} (FaaS).

Die Idee ist, dass Entwickler ihre Anwendungslogik aus isolierten und 
zustandslosen Funktionen aufbauen. Es ist vorgesehen, dass jede Funktion nur für eine
ganz spezifische Aufgabe zuständig ist. Die Funktionen können
durch Ereignisse (engl. \textit{Events}) ausgelöst werden. Treffen solche Ereignisse ein,
provisioniert der Cloudanbieter die passenden Ressourcen und führt die jeweilige Funktion aus.
Aufgrund des geringen Aufgabenbereichs und Zustandslosigkeit einer Funktion,
sind Cloudanbieter in der Lage sehr schnell mehrere Instanzen einer Funktion zu starten,
um eine erhöhte Nachfrage zu bewältigen.
Verringert sich die Nachfrage wieder, werden die Instanzen abgebaut. Anders ist 
es bei traditionellen selbstverwalteten Diensten. Dort müssen Entwickler die Nachfrage grob
abschätzen und bei zu hoher Nachfrage die Kapazitäten rechtzeitig
erhöhen \cite{WhatIsServerless} \cite{ServerlessTrends}.

Ein weiterer Vorteil des Serverless-Computing ist, dass Cloudanbieter
bei ihren serverlosen Diensten nur die eigentlichen Rechentätigkeiten
in Zahlung stellen, sodass für Entwickler und Unternehmen nur dann Kosten
anfallen, wenn ihre Anwendungen auch benutzt werden \cite{EcoArc}.

Auch das Wegfallen der Provisionierung und täglichen Verwaltung bringt Vorteile mit sich.
Entwickler können sich auf die eigentliche Entwicklung der Anwendungen konzentrieren und
müssen sich nicht mit den täglichen Kleinigkeiten des Entwicklungsalltags
beschäftigen \cite{ServerlessTrends}.

Somit lässt sich das Serverless-Computing Konzept gut auf Applikationen anwenden,
die über den Tag hinaus eine starke Schwankung in der Nachfrage haben. Der Cloudanbieter wird
die angefragten Funktionen passend hoch und runter skalieren. Auch wenn bereits stark auf 
andere Services eines Cloud-Providers zurückgegriffen wird, sind die serverlosen Dienste
gut für das Verknüpfen der Services geeignet. Da die einzelnen Funktionen durch Events
aktiviert werden, können Daten in eine Funktion eingeführt werden, um
dann an die benötigten Komponenten, in passendem Dateiformat, wieder ausgeführt zu werden
\cite{ServerlessTrends} \cite{HpcServerless}.

Um in den folgenden Kapiteln der Arbeit tiefer in die Architektur einer serverlosen Platform
eintauchen zu können, müssen die zwei Begriffe Virtual-Machine und Container erklärt werden.

\section{Virtual-Machine}
Bei einer Virtual-Machine handelt es sich um eine virtuelle Umgebung, mit 
eigenen virtuellen Ressourcen, wie unter anderem CPU und Speicher.
Möglich wird die Virtualisierung durch den \textit{Virtual Machine Monitor} (VMM),
auch \textbf{Hypervisor} genannt.
Der Hypervisor ist ein Softwarestück, der dafür zuständig ist die 
virtuellen Ressourcen zu verwalten und Instruktionen aus der virtuellen Umgebung
auf die physischen Ressourcen abzubilden. Er reserviert physische Ressourcen und weist es den VMs zu.
Der Hypervisor ist der Kommunikationskanal zwischen den virtuellen Umgebungen und der Hardware.
Ein großer Vorteil der Virtualisierung von Hardwareressourcen ist,
dass jede Virtual-Machine ihr eigenes Betriebssystem benutzen kann. Somit können auf einer Maschine
mehrere Betriebssysteme laufen.
\cite{RedHatVM} \cite{RedHatHypervisor}.

Es wird grob unter zwei Arten von Hypervisoren unterschieden.
Ein nativer Hypervisor, auch \textbf{Typ 1 Hypervisor} genannt, läuft direkt auf der physischen Hardware
und verwaltet die virtuellen Umgebungen und ihre Betriebssysteme. Ein bekannter Typ 1 Hypervisor ist
unter anderem Microsoft Hyper-V
\footnote{\url{https://docs.microsoft.com/de-de/virtualization/hyper-v-on-windows/}}
\cite{RedHatHypervisor}.
Ein hosted Hypervisor, auch \textbf{Typ 2 Hypervisor}  genannt, läuft als übliche
Applikation auf dem Host-Betriebssystem. Beispiel hierfür wäre Oracle VirtualBox
\footnote{\url{https://www.virtualbox.org/}}, welches auch in der Informatik
an der Heinrich-Heine-Universität (HHU) verwendet wird \cite{HHUFachschaft}.
Der Hypervisor kommuniziert mit dem Host-Betriebssystem und hat keinen direkten Zugriff auf die
physischen Ressourcen. Erst in Zusammenarbeit mit dem Host-Betriebssysteme
werden die Instruktionen an die physischen Hardwareressourcen weitergeleitet.
Somit laufen die virtuellen Umgebungen auf dem Host-Betriebssystem,
haben aber jeweils ein eigenes Betriebssystem \cite{RedHatHypervisor}.
Ein Hosted Hypervisor ist im Vergleich zu einem nativen Hypervisor ressourcen 

\section{Container}
Ein Linux Container beinhalten fast vollständig isolierte Prozesse.
Durch eine Konfigurationsdatei (\textit{Image}) werden alle nötigen Ressourcen und Dateien,
die für das Ausführen der Prozesse benötigt werden, definiert. Ähnlich wie ein Typ 2 Hypervisor
laufen Container auf dem Host-Betriebssystem, mit dem großen Unterschied, dass die einzelnen
Container selbst keine eigenen Betriebssysteme haben. Container teilen sich das
Host-Betriebssystem und \textit{Kernel}.
Dadurch haben Container den Vorteil, dass sie kompakter und viel sparsamer sind.

1. Was sind Container?
-- Verpacken und isolieren von Software in einer produktionsreifen Laufzeitumgebung\\
-- Erleichtert das Entwickeln auf unterschiedlichen Systemen und Umgebungen\\
-- Keine vollständige Isolation\\
-- Host und Container können sich Ressourcen teilen\\
-- Relative ressourcenarm\\

\section{Function as a Service}
-- Ein Cloud Computing Service wie PaaS, IaaS, SaaS\\
-- Kleine, stateless Funktionen die einzelnd abgerechnet und skaliert werden\\

\section{Serverless Containers}
-- Laufzeitumgebung der serverlosen Anwendung wird durch Dockerimage bereitgestellt
-- Anwendung kann beliebigen aufbau haben

\section{Open-Source und self-hosted Lösungen}
-- OpenFaaS
-- IBM OpenWhisk
-- Oracle Fn Project

\section{Sichere Umgebungen}
-- VMs sind zu Ressourcen intensiv
-- Container sind nicht sicher

\subsection{Google gVisor}
-- Applikationskernel für Container in Golang\\
-- Implementiert viele des Linux kernel interfaces\\
-- Fängt SystemCalls ab und gibt sich als GuestKernel aus\\
% TODO: siehe gVisor Doku und Whitepapers\\

\subsection{AWS Firecracker}
-- Benutzt KVM (Kernel-based Virtaul Machine, ähnlich sVMs)\\
% TODO: https://assets.amazon.science/96/c6/302e527240a3b1f86c86c3e8fc3d/firecracker-lightweight-virtualization-for-serverless-applications.pdf

\section{Eigenschaften serverloser Architekturen}
-- Architekturdartstellung einer simplen Webanwendung, Frontend + Backend + Datenbank
-- Vier eigenschaften des Serverless Computing bei jedem der Services deutlich machen
-- Serverless sieht vor soviel wie möglich die Dienste der Cloud-Provider zuverwenden