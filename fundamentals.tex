\chapter{Grundlagen}
Für das Verständnis der Ausarbeitung werden einige Grundlagen benötigt.
In diesem Kapitel werden diese Grundlagen übermittelt.
Es werden die wichtigsten Eigenschaften des Serverless-Computing erläutert,
sowie gängige serverlose Dienste vorgestellt.

\section{Serverless-Computing}
Serverless-Computing (Serverless) ist ein Konzept, mit dem man skalierbare Anwendungen
entwickeln und ohne komplexe Serververwaltung bereitstellen kann \cite{CioGov}.
Das Konzept ist bei den öffentlichen Cloud-Providern beliebt, die viele
serverlose Dienste für Entwickler und Unternehmen bereitstellen. Trotz des Namens
werden selbstverständlich immer noch \emph{Server} verwendet. Der Name symbolisiert die geringe
und einfache Serververwaltung im Vergleich zu üblichen selbstverwalteten
Diensten \cite{CNCF}. Es wird meist zwischen zwei Arten von
serverlosen Diensten unterschieden. Cloud-Provider bieten mehrere
Laufzeitumgebungen und Programmiersprachen zur Auswahl an,
sodass Entwickler durch isolierte, kompakte und zustandslose Funktionen
ihre Anwendungen entwickeln können. Diese Dienstart wird auch
\emph{Function-as-a-Service} (FaaS) genannt, wie beispielsweise
AWS Lambda\footnote{\url{https://aws.amazon.com/de/lambda/}}.
Serverless ist ein relativ neues Thema, sodass
der Begriff immer weiter ausgebaut wird \cite{ServerlessTrends}.
So haben Cloud-Provider mit \emph{Serverless Containers}
eine weitere serverlose Dienstarten eingeführt.
Google Cloud Run\footnote{\url{https://cloud.google.com/run}}
gibt Entwicklern die Freiheit die Laufzeitumgebung ihrer Anwendung
selbst zu definieren, denn für die Bereitstellung erwartet es
ein Dockerimage\footnote{\url{https://docs.docker.com/get-started/overview/}}.
Ein Dockerimage ist eine mehrschichtige Schablone, welche
alle nötigen Abhängigkeiten und Instruktionen für das
Erzeugen eines Container enthält
\cite{DockerDocs} \cite{RedHatDockerImage}.
Entwickler können somit die Laufzeitumgebung ihrer Anwendungen
genaustens definieren. Sie sind nicht gezwungen
ihre Anwendungen aus einzelnen Funktionen zu entwickeln,
sondern können auch eine übliche komplexere Applikation bereitstellen
\cite{ServerlessTrends}.

\subsection{Function as a Service}
Häufig wird der Begriff serverless mit einer ganz bestimmten serverlosen Dienstart
assoziiert, den Function-as-a-Service (FaaS).
Die Idee ist, dass Entwickler ihre Anwendungslogik aus isolierten und 
zustandslosen Funktionen aufbauen.
Es ist vorgesehen, dass jede Funktion nur für eine
ganz spezifische Aufgabe zuständig ist. Die Funktionen können
durch Ereignisse (engl. \emph{Events}) ausgelöst werden.
Durch Events können einzelnen Funktionen miteinander oder mit
weiteren Diensten der Architektur verbunden werden. 
Treffen solche Ereignisse ein, provisioniert der
Cloudanbieter die passenden Ressourcen und führt die jeweilige Funktion aus.
Aufgrund des geringen Aufgabenbereichs und der Zustandslosigkeit einer Funktion,
sind Cloudanbieter in der Lage sehr schnell mehrere Instanzen einer Funktion zu starten,
um eine erhöhte Nachfrage zu bewältigen.
Verringert sich die Nachfrage wieder, werden die Instanzen abgebaut. Anders ist 
es bei traditionellen selbstverwalteten Diensten. Dort müssen Entwickler die Nachfrage grob
abschätzen und bei zu hoher Nachfrage die Kapazitäten rechtzeitig
erhöhen. Verringert sich die Nachfrage wieder muss daran
gedacht werden die Kapazitäten zu verkleinern, da sonst
Ressourcen in Zahlung gestellt werden die nicht gebraucht wurden
\cite{WhatIsServerless} \cite{ServerlessTrends}.

\subsection{Serverless Containers}
Eine weitere serverlose Dienstart sind die Serverless Containers.
Ein großer Nachteil von FaaS Diensten der Cloud-Provider ist,
dass man die Laufzeitumgebung nicht erweitern kann. Bei der
Entwicklung seiner Funktionen muss man aus einer geringen Auswahl
an Programmiersprachen und Laufzeiten auswählen. Serverlose Container
bieten hier einen großen Vorteil, denn sie erwarten ein Dockerimage.
Dadurch haben Entwickler mehr Kontrolle und Freiheit bei der
Anwendungsentwicklung. Zusätzlich bleiben 
die Vorteile der FaaS Dienste erhalten.

\subsection{Eigenschaften von Serverless}
Damit ein Dienst dem Serverless-Prinzip folgt, sollte es
gewisse Charakteristiken aufweisen \cite{ShafKhonMou}. Diese Eigenschaften gelten
für FaaS sowie für Serverless Containers, können jedoch von Cloud-Provider zu
Cloud-Provider leicht abweichen.

\paragraph{Einfache Bereitstellung} Eine Anwendung sollte so einfach
wie möglich bereitgestellt werden können.
FaaS und Serverless Container Dienste erwarten meist nur die
Anwendungslogik und eine Dockerfile.
Mit nur wenigen Kommandos und Interaktionen in der Kommandozeile oder
der grafischen Benutzeroberfläche sollte die Anwendung
bereitgestellt werden können \cite{ServerlessTrends}.

\paragraph{Einfache Verwaltung und Wartung} 
Nach dem Aufsetzen sollte die weitere Verwaltung der Anwendung, wie das Aktualisieren,
ohne Komplexität möglich sein. Google Cloud Run gibt Entwicklern die Option
Containerressourcen, wie beispielsweise die Speicherkapazität \cite{CloudRunMemLimits}
und CPU-Anzahl \cite{CloudRunCpuAlloc}, anzupassen. Die Dienste und darunterliegende
Infrastruktur wird von den Cloud-Providern selbst auf Sicherheitslücken überwacht \cite{ServerlessTrends}.

\paragraph{Automatisches Skalieren} Bei erhöhter Nachfrage sollten weitere Instanzen der Anwendung
schnell genug gestartet werden, sodass die Nachfrage gedeckt wird.
Verringern sich die Anfrage, so sollten die zusätzlich gestarteten Instanzen wieder abgebaut werden.
Durch das automatische Skalieren passt sich der Dienst an die aktuelle Nachfrage der Anwendung an.
Werden gar keine Anfragen an die Anwendung getätigt, soll keine Instanz laufen.
Dies wird auch \emph{scale-to-zero} genannt \cite{ServerlessTrends}.

\paragraph{Bezahlung des eigentlichen Verbrauchs} Kosten fallen nur dann an, wenn mindestens eine
Instanz der Anwendung arbeitet. Oft wird die eigentliche Rechendauer, Anzahl der
Aufrufe pro Monat sowie die Konfiguration der Instanzen in Zahlung gestellt \cite{ServerlessTrends}.
Verarbeitet die Anwendung keine Anfragen, fallen keine Kosten an.
Selbstverwaltete Dienste dagegen werden oft stündlich oder monatlich in Zahlung gestellt.
Dies ist die \emph{pay-for-use} Eigenschaft von Serverless \cite{Firecracker}.
Zusätzlich bieten Google Cloud Run und AWS Lambda auch ein kostenloses Kontingent an
\cite{CloudRunPricing} \cite{AWSLambda}. Erst nachdem die jeweiligen Ressourcen
ausgelastet sind, fallen kosten an.

Somit lässt sich das Serverless-Computing Konzept gut auf Applikationen anwenden,
die über den Tag hinaus eine starke Schwankung in der Nachfrage haben. Der Cloudanbieter wird
die angefragten Funktionen passend hoch und runter skalieren.
Dies kann unter Umständen auch zu geringeren Kosten führen 
Auch wenn bereits stark auf 
andere Services eines Cloud-Providers zurückgegriffen wird, sind die serverlosen Dienste
gut für das Verknüpfen der Services geeignet. Da die einzelnen Funktionen durch Events
aktiviert werden, können Daten in eine Funktion eingeführt werden, um
dann an die benötigten Komponenten, in passendem Format, wieder ausgeführt zu werden
\cite{ServerlessTrends} \cite{HpcServerless}.

\subsection{Nachteile von Serverless}
Neben den zahlreichen Vorteilen bergen serverlose Dienste auch einige Nachteile.
Auch die Nachteile gelten, mit einer Ausnahme, für FaaS sowie für
Serverless Containers Dienste.

\paragraph{Abhängigkeit der Cloud-Provider} Das Verwenden von vielen serverlosen Diensten kann zur Abhängigkeit der
Cloud-Provider (engl. \emph{vendor lock-in}) führen. Die einzelnen
Services lassen sich leicht miteinander verbinden,
was weiter die Komplexität der gesamten Anwendungsarchitektur verringert. Jedoch erwarten
FaaS Dienste unterschiedliche Funktionsschreibweisen, sodass der Wechsel zu einem
anderen Anbieter nicht mühelos ist \cite{EcoArc}.
Serverless Container haben dieses Problem nicht.
Deren Laufzeitumgebung wird durch ein Dockerimage festgelegt.
Somit ist der Wechsel zu anderen Diensten ohne großen Aufwand
möglich.

\paragraph{Flüchtige Instanzen} Die gestarteten Instanzen sind flüchtig (engl. \emph{ephemeral}). Dadurch
eignet sich Serverless nicht für alle Anwendungen. Anfragen dürfen nicht davon ausgehen, dass sie von der
gleichen Instanz bearbeitet werden. Einige Dienste, wie Google Cloud Run,
haben auch eine maximale Zeitspanne (engl. \emph{timeout}), in der eine Anfrage abgearbeitet werden muss.
Dauert es länger, wird die Bearbeitung abgebrochen \cite{CloudRunTimeout}.

\paragraph{Cold Starts} Ein weiteres Problem von Serverless sind die \emph{Cold-Starts}.
Bei einer eintretenden Anfrage, braucht das Hochfahren einer neuen Anwendungsinstanz
gewisse Zeit, denn Anwendungslogik und Laufzeitumgebung müssen erst initialisiert werden.
Cloud-Provider versuchen diese Zeit zu minimieren \cite{Firecracker}. Wird eine Instanz mit der Bearbeitung einer Anfrage
fertig, so wird sie nicht sofort herunterfahren, sondern erst nach einer gewissen Zeit.
Dadurch kann sie bei erneuter Anfrage während diese Zeit wiederverwendet werden, denn Laufzeitumgebung und
Anwendungslogik sind bereits geladen \cite{ColdStartComp}.

\subsection{Open-Source Serverless Platform}

Neben den serverlosen Diensten der Cloud-Provider stehen
auch Open-Source Lösungen zur Verfügung. Platformen wie
OpenFaaS\footnote{\url{https://www.openfaas.com/}},
Apache OpenWhisk\footnote{\url{https://openwhisk.apache.org/}} und
Oracle Fn Project\footnote{\url{https://fnproject.io/}} geben
Entwicklern die Möglichkeit ihre Applikationen
nach dem Serverless Konzept bereitzustellen und
von den Vorteilen zu profitieren. Wie bei Serverless
Container Diensten, haben sie den Vorteil,
dass die Laufzeitumgebung der Instanzen durch
eine Dockerfile konfiguriert werden kann.
Jedoch werden einzelne ereignisgesteuerte Funktionen benutzt,
um Anwendungen zu entwickeln, ähnlich den FaaS Diensten der
Cloud-Provider \cite{OpenFaasGithub} \cite{OpenWhiskGithub}.
Einen großen Nachteil haben sie dennoch.
Für das Ausführen in einer Produktionsumgebung setzen einige
der Platformen ein Container-Orchestrierungswerkzeug vorraus.
Dieser muss erst aufgesetzt werden oder Entwickler greifen auf
einen verwalteten Dienst der Cloud-Providern zurück. Jedoch fallen
dann stündliche bzw. monatliche Fixkosten an und die
Verwaltung der Anwendung steigt, sodass die Vorteil
von Serverless nicht ganz ausgeschöpft werden.

\subsection{Aufbau einer serverless Platform}
Obwohl die ausgearbeitete Anwendung im folgenden Kapitel
mithilfe von Google Cloud Run entwickelt wird,
wird hier der Aufbau einer serverlosen Platform
anhand von Apache OpenWhisk vorgestellt.
Aus Googles Dokumentationen wird nicht eindeutig ersichtlich, wie
deren Service aufgebaut ist. Dagegen ist OpenWhisk eine
gut dokumentierte Open-Source Lösung.

In OpenWhisk werden serverlose Funktionen \emph{Actions} genannt.
Dafür wird in einer Datei eine Funktion deklarieren.
Die Funktion muss gewisse Vorraussetzungen erfüllen,
damit OpenWhisk sie verwenden kann.
Unteranderem muss der Rückgabewert als JSON Objekt
interpretiert werden können \cite{OpenWhiskGithubActions}.\\

\begin{lstlisting}[caption={Hello World JS Funktion. In Anlehnung an \cite{OpenWhiskGithub} \cite{OpenWhiskGithubActions}}, label={lst:openwhisk_action}]
function main(params) {
    console.log('Hello World');
    return { msg: 'Hello World' };
}
\end{lstlisting}

Die Funktion in Auflistung~\ref{lst:openwhisk_action},
die nach Ausführung den Text \texttt{Hello World}
in der Standardausgabe ausgibt und selbigen Text
auch als JSON-Objekt zurück sendet, erfüllt alle Vorraussetzungen
für die Umwandlung zu einer Action.\\

\begin{lstlisting}[caption=Erstellen einer Action \cite{OpenWhiskGithub}, label={lst:openwhisk_create_action}]
wsk action create myAction action.js
\end{lstlisting}

Die Action wird mithilfe der OpenWhisk CLI (\texttt{wsk})
und dem Zusatz \texttt{action create} erzeugt.
Mit \texttt{myAction} und \texttt{action.js} 
in Auflistung~\ref{lst:openwhisk_create_action}
sind der Name der Action und der Name der Datei angegeben
\cite{OpenWhiskGithub}.\\

\begin{lstlisting}[caption=Action Endpunkt \cite{OpenWhiskGithub}, label={lst:openwhisk_action_endpoint}]
/api/v1/namespaces/$userNamespace/actions/myAction
\end{lstlisting}

Erstellen Entwickler ihre Actions werden sie durch
einen HTTP-Endpunkt der Form in Auflistung~\ref{lst:openwhisk_action_endpoint},
zugänglich.
Die Variable \texttt{\$userNamespace} ist hier ein
Platzhalter für einen Namensraum.
In OpenWhisk können Actions für eine bessere
Verwaltung und Authorisierung in Namensräumen aufgeteilt werden
\cite{OpenWhiskGithub}.

\begin{figure}
    \centering
    \includegraphics[height=10cm]{fig/openwhisk_architecture}
    \caption{Apache OpenWhisk Architektur. In Anlehnung an \cite{OpenWhiskGithub}}
    \label{fig:openwhisk_architecture}
\end{figure}

In Abbildung~\ref{fig:openwhisk_architecture} ist die Architektur von
OpenWhisk abgebildet.
Sie baut auf unterschiedliche Komponenten auf,
um eine Anfrage zu bearbeiten. Bei einem eintrettenden Event \textbf{(1)}
ist \emph{Nginx}\footnote{\url{https://www.nginx.com/}} \textbf{(2)} die erste
Station. Diese wird zur SSL-Terminierung und Weiterleitung verwendet.
Nginx leitet den Aufruf weiter an den \emph{Controller} \textbf{(3)}.
Der Controller implementiert die Rest-API und filtert die
eintrettenden Anfragen durch, um die passende Action und
den weiteren Verlauf zu ermitteln.
Er authorisiert die Anfrage und schaut auf nötigen Authentifizierungen.
Dafür befragt er CouchDB\footnote{\url{https://couchdb.apache.org/}}
\textbf{(4)},
eine dokumentenbasierte Datenbank. Nachdem Authorisierungs-
und Authentifizierungsschritte erfolgreich abschließen,
wird die Actions-Logik aus der Datenbank entnommen \textbf{(4)} und an den
Load-Balancer weitergeleitet. Der Load-Balancer ist bestandteil
des Controllers und enthält eine Sicht auf alle \emph{Invoker}.
Die Aufgabe der Invoker ist es, die Action am Ende auszuführen.
In der Theorie würde der Load-Balancer sich einen freien Invoker suchen
und ihm die Action-Logik übergeben. Jedoch kann es zu Unerwarteten
Störungen und Abstüzungen kommen oder es lässt sich kein freier
Invoker finden. Deshalb sendet der Load-Balancer eine Nachricht an
Apache Kafka\footnote{\url{https://kafka.apache.org/}} \textbf{(5)},
einer verteilten Streaming-Platform. Kafka speichert seine
einkommenden Nachrichten, sodass auch bei einem Systemabsturz alle
Nachrichten vorhanden sind. Die Nachricht enthält welcher
Invoker die Action ausführen soll, auch wenn dieser momentan
beschäftigt ist. Kafka verarbeitet die Nachricht und sendet eine
Empfangsbestätigung mit der \texttt{activationId} zurück. Mithilfe
der \texttt{activationId} lässt sich später das Result der Action
abfragen. Wenn der Invoker aus der Nachricht frei ist, wird die
Action ausgeführt. Dafür wird ein Docker Container gestartet \textbf{(6)},
welcher eine passende Laufzeitumgebung für die Action zur Verfügung
stellt. Die Action-Logik wird in den Container geladen und ausgeführt.
Das Resultat wird entnommen und der Container wird heruntergefahren.
Das Resultat sowie die \texttt{activationId} werden in die
\texttt{activations}-Datenbank \textbf{(7)} gespeichert. Nun kann, wie bereits zu Beginn,
über den Endpunkt und der \texttt{activationId} das Resultat aus
der \texttt{activations}-Datenbank abgerufen werden werden
\cite{OpenWhiskGithub}.

\section{Sichere Umgebungen für Serverless}
Für Cloud-Provider ist es wichtig, dass die einzelnen
Anwendungen ihrer zahlreichen Kunden sicher und abgekapselt
voneinander laufen, sodass niemand unbefugten Zugriff auf
andere Anwendungen hat. In diesem Abschnitt werden
Virtual-Machines und Container näher erläutert, um ein Verständnis
zu bekommen wie die einzelnen Anwendungen untereinander abgesichert 
sind und ob sie für serverlose Anwendungen gut geeignet sind.

\subsection{Virtual-Machine}
Bei einer Virtual-Machine handelt es sich um eine virtuelle Umgebung, mit 
eigenen virtuellen Ressourcen, wie unter anderem CPU und Speicher.
Ein \emph{Virtual Machine Monitor} (VMM), auch \emph{Hypervisor} genannt
ist ein Softwarestück, das dafür zuständig ist die 
virtuellen Umgebungen einschließlich seiner virtuellen Ressourcen zu verwalten
und Instruktionen auf die physischen Ressourcen abzubilden.
Der Hypervisor reserviert physische Ressourcen und plant die Zuweisen
an die einzelnen VMs. Er ist der Kommunikationskanal 
zwischen den virtuellen Umgebungen und der Hardware.
Ein großer Vorteil der Virtualisierung von Hardwareressourcen
ist, dass jede Virtual-Machine ihr eigenes Betriebssystem
benutzen kann. Somit können auf einer Maschine
mehrere Betriebssysteme laufen, die vollständig voneinander abgekapselt sind
\cite{RedHatVM} \cite{RedHatHypervisor}. Es wird unter zwei
Arten von Hypervisor unterschieden.

\paragraph{Typ 1 Hypervisor} Ein nativer Hypervisor,
auch \emph{Typ 1 Hypervisor} genannt, läuft direkt auf der physischen
Hardware und verwaltet die virtuellen Umgebungen und ihre Betriebssysteme.
Der Hypervisor benötigt selbst kein zugrundeligendes Betriebssystem.
Gängige Typ 1 Hypervisor sind unter anderem Microsoft Hyper-V\footnote{\url{https://docs.microsoft.com/de-de/virtualization/hyper-v-on-windows/}}
\cite{RedHatHypervisor} und die Kernel-basierte Virtual
Machine (\emph{KVM}). Die KVM ist in den Linux-Kernel integriert und wandelt
das Linux-Betriebssystem in einen Typ 1 Hypervisor. Sie unterstützt
Hardware-Virtualisierungserweiterungen. Diese Erweiterungen wurden eingeführt,
um die Virtualisierung und die Entwicklung von Hypervisor
zu beschleunigen. KVM kann auch mit einem weiteren
Typ 2 Hypervisor benutzt werden, um diesen zu unterstützen und
zu beschleunigen \cite{IBMHypervisor} \cite{RedHatKVM}.

\paragraph{Typ 2 Hypervisor} Ein hosted Hypervisor, auch
\emph{Typ 2 Hypervisor}  genannt, läuft als übliche
Applikation auf dem Host-Betriebssystem. Beispiel hierfür wäre
Oracle VirtualBox\footnote{\url{https://www.virtualbox.org/}},
welches auch in der Informatik
an der Heinrich-Heine-Universität (HHU) verwendet wird \cite{HHUFachschaft}.
Der hosted Hypervisor kommuniziert mit dem Host-Betriebssystem und hat keinen direkten Zugriff auf die
physischen Ressourcen. Erst in Zusammenarbeit mit ihm
werden die Instruktionen an die physischen Hardwareressourcen weitergeleitet.
Somit laufen die virtuellen Umgebungen auf dem Host-Betriebssystem,
haben aber jeweils ein eigenes Betriebssystem \cite{RedHatHypervisor}.
Im Vergleich zu einem nativen Hypervisor ist das Erstellen einer VM
mit einem hosted Hypervisor ohne großen Aufwand möglich.
Ein Nachteil ist jedoch, dass sie langsamer und ineffizienter sind, da sie nicht
direkt mit der Hardware kommunizieren \cite{IBMHypervisor}.

\subsection{Container}
Virtualisierung ist auch mithilfe von Linux Container möglich,
was auch Containerisierung (engl. \emph{Containerization})
genannt wird. Diese enthalten isolierte Prozesse.
Container teilen sich den Host-Kernel. Durch eine
Konfigurationsdatei (\emph{Image}) werden alle
nötigen Ressourcen und Dateien definiert, die für das Ausführen
der Prozesse benötigt werden. Dadurch spart man sich ein komplettes
Betriebsystem zu laden, wodurch sie den Vorteil,
dass sie kompakter und sparsamer sind. Entwickler steht eine eindeutig
definierte Laufzeitumgebung zur Verfügung, auf der sie ihre Anwendungen testen
können \cite{RedHatContainer}. Container bauen auf drei Komponenten
des Linux-Kernels auf, den Namensräumen
(\emph{namespaces}\footnote{\url{http://manpages.ubuntu.com/manpages/focal/man7/namespaces.7.html}}),
den Kontrollgruppen
(\emph{cgroups}\footnote{\url{http://manpages.ubuntu.com/manpages/focal/en/man7/cgroups.7.html}}).
und den Filtern (\emph{seccomp-bpf}\footnote{\url{http://manpages.ubuntu.com/manpages/focal/man2/seccomp.2.html}}).

Mithilfe von namespaces stellt der Kernel eine Isolationsmöglichkeit
für Prozesse bereit. Mit ihnen können globale Ressourcen abstrahiert werden.
Prozesse innerhalb des namespaces sehen nur ihre isolierte lokale
Ausprägung der Ressource \cite{UbuntuNamespaces}.
Dadurch können mehrere Container gleichzeitig auf die gleiche
Ressource zugreifen \cite{RedHatIntroToLinuxContainers}.
Es existieren unterschiedliche namespaces, wie beispielsweise
\emph{mount namespaces}, \emph{network namespaces}
und \emph{pid namespaces}.
Durch mount namespaces kann die Sicht auf das Dateisystem
(engl. \emph{filesystem}) für Prozesse beschränkt werden.
Mit network namespaces lassen sich IP-Adressen, Ports
und Routing Tabellen isolieren. PID (deut. Prozess-ID) namespaces teilen
den Identifikationsbereich. Ein Prozess hat dadurch zwei
PIDs. Eine Identifaktionsnummer auf dem Host-Betriebssystem und eine
innerhalb des namespaces, welche sich beide unterscheiden können
\cite{LwnDotNetNamespaces}.

Es gibt zwei Aufgaben, die cgroups erfüllen. Sie erlauben das Gruppieren
von Prozessen und können dynamisch Ressourcen für Gruppierungen begrenzen.
Begrenz werden können beispielsweise CPU Zeit und Speicher eines Prozesses.
Möglich wird dies durch \emph{subsystems},
auch \emph{resource controller} genannt. Subsytems sind Bestandteil
des Kernels und erlauben das Ändern von Prozesseigenschaften und
Prozessverhalten in einer cgroups Gruppierung. 
\cite{RedHatIntroToLinuxContainers}.

Für die Sicherheit von Containerinstanzen ist secomp-bpf 
verantwortlich. Es filtert und limitiert
Systemaufrufe (\emph{syscalls}) von Prozessen,
sowie die mitgelieferten Parameter der Systemaufrufe.
Secomp-bpf ist ein wichtigster Baustein in der Isolation von
Containern. Es verkleinert die Angriffsfläche 
des Kernels. Jedoch benutzen Container einen
Kernel, sodass bei Ausnutzung von Sicherheitslücken
des Kernels alle Container angreifbar sind \cite{Firecracker}.

\subsection{Ideale Virtualisierung für Serverless}
Durch Serverless ist es möglich,
dass eine Applikation in kürzester Zeit
auf hunderte Instanzen hochskaliert. Vorallem bei FaaS Diensten,
wo eine Funktion nur eine ganz spezifische Aufgabe tätigt,
kann es vorkommen, dass eine Anwendung aus mehreren hunderten
von kleinen Funktionen aufgebaut ist. Diese können alle separat
skaliert werden können. Traditionelle VMs bieten
eine robuste Isolation, denn jede VMs hat einen eigenen
Guest-Kernel und ist durch den Hypervisor
abgekapselt vom Host-Kernel und anderen VMs. Sie haben jedoch
eine großen Startaufwand und sind für Serverless zu
ressourcenintensiv \cite{Firecracker}. Container dagegen
haben einen geringeren Startaufwand. Die Laufzeitumgebung der
Anwendung lässt sich genaustens definieren, sodass keine
unnötigen Ressourcen verschwendet werden. Aber Container
teilen sich den Host-Kernel, sodass eine Sicherheitslücke
im Kernel die Container angreifbar macht.
Die ideale Virtualisierungmethode für Serverless sollte
gewisse Eigenschaften aufweisen \cite{Firecracker}.

\paragraph{Isolation} Mehrere Anwendungen, die gleichzeitig
auf der selben Hardware laufen, sollten abgesichert und
isoliert voneinander sein. Die Warhscheinlichkeit auf
Sicherheitslücken und unbefugten Zugriff soll verringert werden
\cite{Firecracker}.

\paragraph{Aufwand und Dichte} Anders als traditionelle VMs
sollte die ideale Virtualisierungmethode Anwendungen
nicht zusätzlich verlangsamen. Die sichere Umgenung sollte
kompakt sein \cite{Firecracker}.

\paragraph{Performance} Die Leistung der Anwendungen sollten
nicht beeinträchtigt werden. Zusätzlich sollte die Leistung auch 
konstant und unabhängig von den einzelnen Instanzen sein
\cite{Firecracker}.

\paragraph{Kompatibilität} Anwendungen sollten die Möglich haben,
ohne Einschränkungen beliebige Bibiliotheken und Dateien zu
verwenden. Anwendungen sollten nicht extra angepasst werden
müssen, um von der Sicherheit und Isolation zu profitieren
\cite{Firecracker}.

\paragraph{Schnelles Umschalten} Die Skalierungseigenschaft von
Serverless sollte durch die Virtualisierungmethode nicht
beeinträchtigt werden. Mehrere Instanzen der Anwendung sollten
zügig hoch- und runtergefahren werden können \cite{Firecracker}.

\paragraph{Soft Allocation} Da FaaS und Serverless
Container Dienste auch nach Speicher,
CPU und anderen Ressourcen konfiguriert
werden können, muss die Virtualisierungmethode eine
Überprovisionierung diese Ressourcen erlauben. Jedoch sollte
eine Instanz nur die nötigen Ressourcen in Anspruch nehmen
\cite{Firecracker}.

\subsection{AWS Firecracker}
Nach diesem theoretischen Ideal hat AWS mit Firecracker
eine eigene Virtualisierungmethode aufgebaut, um ihre
serverlosen Dienste zu verbessern.
Firecracker ist ein Typ 2 Hypervisor optimiert für Serverless
und entwickelt in Rust\footnote{\url{https://www.rust-lang.org}}.
Mithilfe der KVM im Linux-Kernel erzeugt es kompakte
Virtual-Machines (\emph{MicroVMs}). Möglich wird dies durch das
Entfernen von Funktionen üblicher Hypervisor. Es werden nur wenige
Geräte ünterstüzt. Geräte-Treiber für USB und
GPUs wurden entfernt. Firecracker läuft als Prozess, der durch seccomp,
cgroups und namespaces
eingeschlossen ist und nur begrenzten Zugriff auch die
Hardware und das Dateisystem hat. 
AWS hat Firecracker
auch eine REST-API beigelegt. Mit ihr können die einzelnen
MicroVMs besser gesteuert und verwaltet werden \cite{Firecracker}. 

\subsection{Google gVisor}
Auch Google hat mit gVisor bei der Virtualisierung
ihre Infrastruktur nachgebessert. Google gVisor ist ein
Applikationskernel \cite{gVisor}. Es reimplementiert viele
Aspekte des Linux-Kernel in Go\footnote{\url{https://golang.org}}.
Es läuft als Prozess und nimmt den Platz des Guest-Kernel ein
auf den übliche Anwendungen laufen können, unteranderm auch
isolierte Container.
Ähnlich wie seccomp fängt gVisor alle Systemaufrufe der
Anwendungen ab und versucht viele diese Aufrufe selbst zu bearbeiten.
Nur wenige werden an den Host-Kernel weitergeleitet. Da gVisor
nicht alle Systemaufrufe implementiert wie der Linux-Kernel
kann es für bestimmte Anwendungen zu Kompatibilitätsproblemen
führen \cite{gVisor}.

% \section{Eigenschaften serverloser Architekturen}
% -- Architekturdartstellung einer simplen Webanwendung, Frontend + Backend + Datenbank
% -- Vier eigenschaften des Serverless Computing bei jedem der Services deutlich machen
% -- Serverless sieht vor soviel wie möglich die Dienste der Cloud-Provider zuverwenden