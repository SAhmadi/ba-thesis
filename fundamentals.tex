\chapter{Grundlagen}
Für das Verständnis der Ausarbeitung werden einige Grundlagen benötigt.
In diesem Kapitel werden diese Grundlagen übermittelt.
Es werden die wichtigsten Eigenschaften des Serverless-Computing erläutert,
sowie gängige serverlose Dienste vorgestellt.

\section{Serverless-Computing}
Serverless-Computing ist ein Konzept, mit dem man skalierbare Anwendungen
entwickeln und ohne komplexe Serververwaltung bereitstellen kann \cite{CioGov}.
Das Konzept ist bei den öffentlichen Cloud-Providern beliebt, die viele
serverlose Dienste für Entwickler und Unternehmen bereitstellen. Trotz des Namens
werden selbstverständlich immer noch \textit{Server} verwendet. Der Name symbolisiert die geringe
und einfache Serververwaltung im Vergleich zu üblichen selbstverwalteten
Diensten \cite{CNCF}. Es wird meist zwischen zwei Arten von
serverlosen Diensten unterschieden. Cloud-Provider bieten mehrere
Laufzeitumgebungen und Programmiersprachen zur Auswahl an,
sodass Entwickler durch isolierte, kompakte und zustandslose Funktionen
ihre Anwendungen entwicklen können. Diese Dienstart wird auch
\textit{Function-as-a-Service} (FaaS) genannt, wie beispielsweise
AWS Lambda \footnote{\url{https://aws.amazon.com/de/lambda/}}.
Serverless ist ein relativ neues Thema, sodass
der Begriff immer weiter ausgebaut wird \cite{ServerlessTrends}.
So haben Cloud-Provider mit \textit{Serverless Containers}
eine weitere serverlose Dienstarten eingeführt. Google Cloud Run
\footnote{\url{https://cloud.google.com/run}} und AWS Fargate
\footnote{\url{https://aws.amazon.com/de/fargate/}} geben Entwicklern
die Freiheit eine beliebige Anwendungen bereitzustellen,
denn sie erwarten für die Bereitstellung eine Dockerfile
\footnote{\url{https://docs.docker.com/engine/reference/builder/}}.
Entwickler können somit die Laufzeitumgebung ihrer Anwendungen
genaustens definieren und kontrollieren \cite{ServerlessTrends}.

\subsection{Function as a Service}
Häufig wird der Begriff serverless mit einer ganz bestimmten serverlosen Dienstart
assoziiert, den Function-as-a-Service (FaaS).
Die Idee ist, dass Entwickler ihre Anwendungslogik aus isolierten und 
zustandslosen Funktionen aufbauen. Es ist vorgesehen, dass jede Funktion nur für eine
ganz spezifische Aufgabe zuständig ist. Die Funktionen können
durch Ereignisse (engl. \textit{Events}) ausgelöst werden.
Durch Events können einzelnen Funktionen miteinander oder mit
weiteren Diensten der Architektur verbunden werden. 
Treffen solche Ereignisse ein, provisioniert der
Cloudanbieter die passenden Ressourcen und führt die jeweilige Funktion aus.
Aufgrund des geringen Aufgabenbereichs und der Zustandslosigkeit einer Funktion,
sind Cloudanbieter in der Lage sehr schnell mehrere Instanzen einer Funktion zu starten,
um eine erhöhte Nachfrage zu bewältigen.
Verringert sich die Nachfrage wieder, werden die Instanzen abgebaut. Anders ist 
es bei traditionellen selbstverwalteten Diensten. Dort müssen Entwickler die Nachfrage grob
abschätzen und bei zu hoher Nachfrage die Kapazitäten rechtzeitig
erhöhen. Verringert sich die Nachfrage wieder muss daran
gedacht werden die Kapazitäten zuverkleinern, da sonst
Ressourcen in Zahlung gestellt werden die nicht gebraucht wurden
\cite{WhatIsServerless} \cite{ServerlessTrends}.

\subsection{Serverless Containers}
Eine neue serverlose Dienstart sind die Serverless Containers.
Ein großer Nachteil von FaaS Diensten der Cloud-Provider ist,
dass man die Laufzeitumgebung nicht erweitern kann. Bei der
Entwicklung seiner Funktionen muss man aus einer geringen Auswahl
an Programmiersprachen und Laufzeiten auswählen. Serverlose Container
bieten hier einen großen Vorteil, denn sie erwarten eine Dockerfile.
Eine Dockerfile ist eine Konfigurationsdatei, in der man die
Laufzeitumgebung des Containers genaustens definieren kann.
Dadurch haben Entwickler mehr Kontrolle und Freiheit bei der
Anwendungsentwicklung. 

\subsection{Eigenschaften von Serverless}
Damit ein Dienst dem \textit{Serverless} Prinzip folgt, sollte es
es gewisse Charakteristiken aufweisen. Diese Eigenschaften gelten
für FaaS und Serverless Container.

\paragraph{Einfache Bereitstellung und Verwaltung} Eine Anwendung sollte so einfach
wie möglich bereitsgestellt und verwaltet werden können.
FaaS und Serverless Container Dienste erwarten meist nur die
Anwendungslogik und eine Dockerfile \cite{ServerlessTrends}.
Mit nur wenigen Kommandos und Interaktionen in der Kommandozeile oder
der graphischen Benutzeroberfläche sollte die Anwendung
bereitgestellt werden können. Nach dem Aufsetzen sollte
die weitere Verwaltung der Anwendung, wie Updates und
Anpassung der Ressourcen, den Entwicklern ohne
Komplexität ermöglicht werden. Serverlose Dienste, wie
Google Cloud Run, geben den Entwicklern die Möglichkeit unteranderem
die Speicherkapazität \cite{CloudRunMemLimits} und
CPU-Anzahl \cite{CloudRunCpuAlloc} der Containerinstanzen zu setzen.  

\paragraph{Skalierbarkeit} Eine Anwendung  


% Skalierbar
% (x) Einfache Bereitstellung und Verwaltung
% Zahlung nach Rechenzeit, nicht Stündlich oder monatlich
% 


\subsection{Aufbau einer serverless Platform}
Obwohl die ausgearbeitete Anwendung in Kapitel 3
mithilfe des Serverless Container Dienstes von Google entwickelt wird,
wird im folgenden der Aufbau einer serverlosen Platform
anhand von Apache OpenWhisk
\footnote{\url{https://openwhisk.apache.org/}} vorgestellt.
Aus Googles Dokumentationen wird nicht eindeutig ersichtlich, wie
deren Service aufgebaut ist. Dagegen ist OpenWhisk eine
gut dokumentierte Open-Source Lösung, die initial von IBM entwickelt
wurde.

In OpenWhisks Dokumentation werden serverlose Funktionen
\textit{Actions} genannt. Erstellen Entwickler ihre
Actions werden sie durch einen HTTP-Endpunkt der Form\\
\texttt{/api/v1/namespaces/\$userNamespace/actions/myAction} zugänglich
\cite{OpenWhiskGithub}. \texttt{myAction} ist der Name der Action
und die \texttt{\$userNamespace} Variable ist hier ein
Platzhalter, denn in OpenWhisk können Actions für eine bessere
Verwaltung und Authorisierung in Gruppen aufgeteilt werden.
Im nachfolgenden wird vom Standard-Namespace, dem Unterstrich,
ausgegangen. Der Endpunkt ändert sich zu
\texttt{/api/v1/namespaces/\_/actions/myAction} ab.
Bei einem eintrettenden Event ist \textit{nginx}
\footnote{\url{https://www.nginx.com/}} die erste Station.
Diese wird zur SSL-Terminierung und Weiterleitung verwendet.
Nginx leitet den Aufruf weiter an den \textit{Controller}.
Der Controller implementiert die Rest-API und filtert die
eintrettenden Anfragen durch, um die passende Action und
den weiteren Verlauf zu ermitteln.
Er authorisiert die Anfrage und schaut auf nötigen Authentifizierungen.
Dafür befragt er CouchDB \footnote{\url{https://couchdb.apache.org/}},
eine dokumentenbasierte Datenbank. Nachdem Authorisierungs-
und Authentifizierungsschritte erfolgreich abschließen,
wird die Actions-Logik aus der Datenbank entnommen und an den
Load-Balancer weitergeleitet. Der Load-Balancer ist bestandteil
des Controllers und enthält eine Sicht auf alle \textit{Invoker}.
Die Aufgabe der Invoker ist es, die Action am Ende auszuführen.
In der Theorie würde der Load-Balancer sich einen freien Invoker suchen
und ihm die Action-Logik übergeben. Jedoch kann es zu Unerwarteten
Störungen und Abstüzungen kommen oder es lässt sich kein freier
Invoker finden. Deshalb sendet der Load-Balancer eine Nachricht an
Apache Kafka \footnote{\url{https://kafka.apache.org/}},
einer verteilten Streaming-Platform. Kafka speichert seine
einkommenden Nachrichten, sodass auch bei einem Systemabsturz alle
Nachrichten vorhanden sind. Die Nachricht enthält welcher
Invoker die Action ausführen soll, egal ob dieser momentan
beschäftigt ist. Kafka verarbeitet die Nachricht und sendet eine
Empfangsbestätigung mit der \texttt{activationId} zurück. Mithilfe
der \texttt{activationId} lässt sich später das Result der Action
abfragen. Wenn der Invoker aus der Nachricht frei ist, wird die
Action ausgeführt. Dafür wird ein Docker Container gestartet,
welcher eine passende Laufzeitumgebung für die Action zur Verfügung
stellt. Die Action-Logik wird in den Container geladen und ausgeführt.
Das Resultat wird entnommen und der Container wird heruntergefahren.
Das Resultat sowie die \texttt{activationId} werden in die
activations-Datenbank gespeichert. Nun kann, wie bereits zu Beginn,
über dem Endpunkt und der \texttt{activationId} das Resultat abgefragt
werden \cite{OpenWhiskGithub}.


Ein weiterer Vorteil des Serverless-Computing ist, dass Cloudanbieter
bei ihren serverlosen Diensten nur die eigentlichen Rechentätigkeiten
in Zahlung stellen, sodass für Entwickler und Unternehmen nur dann Kosten
anfallen, wenn ihre Anwendungen auch benutzt werden \cite{EcoArc}.

Auch das Wegfallen der Provisionierung und täglichen Verwaltung bringt Vorteile mit sich.
Entwickler können sich auf die eigentliche Entwicklung der Anwendungen konzentrieren und
müssen sich nicht mit den täglichen Kleinigkeiten des Entwicklungsalltags
beschäftigen \cite{ServerlessTrends}.

Somit lässt sich das Serverless-Computing Konzept gut auf Applikationen anwenden,
die über den Tag hinaus eine starke Schwankung in der Nachfrage haben. Der Cloudanbieter wird
die angefragten Funktionen passend hoch und runter skalieren. Auch wenn bereits stark auf 
andere Services eines Cloud-Providers zurückgegriffen wird, sind die serverlosen Dienste
gut für das Verknüpfen der Services geeignet. Da die einzelnen Funktionen durch Events
aktiviert werden, können Daten in eine Funktion eingeführt werden, um
dann an die benötigten Komponenten, in passendem Format, wieder ausgeführt zu werden
\cite{ServerlessTrends} \cite{HpcServerless}.

Um in den folgenden Kapiteln der Arbeit tiefer in die Architektur einer
serverlosen Platform und den Aspekt der Sicherheit eintauchen zu können,
werden die zwei Begriffe Virtual-Machine und Container vorgestellt.

\section{Virtual-Machine}
Bei einer Virtual-Machine handelt es sich um eine virtuelle Umgebung, mit 
eigenen virtuellen Ressourcen, wie unter anderem CPU und Speicher.
Ein \textit{Virtual Machine Monitor} (VMM), auch \textit{Hypervisor} genannt
ist ein Softwarestück, das dafür zuständig ist die 
virtuellen Umgebungen einschließlich seiner virtuellen Ressourcen zu verwalten
und Instruktionen auf die physischen Ressourcen abzubilden.
Der Hypervisor reserviert physische Ressourcen und plant die Zuweisen
an die einzelnen VMs. Er ist der Kommunikationskanal 
zwischen den virtuellen Umgebungen und der Hardware.
Ein großer Vorteil der Virtualisierung von Hardwareressourcen
ist, dass jede Virtual-Machine ihr eigenes Betriebssystem
benutzen kann. Somit können auf einer Maschine
mehrere Betriebssysteme laufen, die vollständig voneinander abgekapselt sind
\cite{RedHatVM} \cite{RedHatHypervisor}. Es wird unter zwei
Arten von Hypervisor unterschieden.

\subsection{Hypervisortyp}
Ein nativer Hypervisor, auch \textit{Typ 1 Hypervisor} genannt, läuft direkt auf der physischen Hardware
und verwaltet die virtuellen Umgebungen und ihre Betriebssysteme. Ein gängiger Typ 1 Hypervisor ist
unter anderem Microsoft Hyper-V
\footnote{\url{https://docs.microsoft.com/de-de/virtualization/hyper-v-on-windows/}}
\cite{RedHatHypervisor}. Es wird kein zugrundeligendes Betriebssystem
benötigt.

Ein hosted Hypervisor, auch \textit{Typ 2 Hypervisor}  genannt, läuft als übliche
Applikation auf dem Host-Betriebssystem. Beispiel hierfür wäre Oracle VirtualBox
\footnote{\url{https://www.virtualbox.org/}}, welches auch in der Informatik
an der Heinrich-Heine-Universität (HHU) verwendet wird \cite{HHUFachschaft}.
Der hosted Hypervisor kommuniziert mit dem Host-Betriebssystem und hat keinen direkten Zugriff auf die
physischen Ressourcen. Erst in Zusammenarbeit mit ihm
werden die Instruktionen an die physischen Hardwareressourcen weitergeleitet.
Somit laufen die virtuellen Umgebungen auf dem Host-Betriebssystem,
haben aber jeweils ein eigenes Betriebssystem \cite{RedHatHypervisor}.
Im Vergleich zu einem nativen Hypervisor ist das Erstellen einer VM
mit einem hosted Hypervisor ohne großen Aufwand möglich.
Ein Nachteil ist jedoch, dass sie langsamer und ineffizienter sind, da sie nicht
direkt mit der Hardware kommunizieren \cite{IBMHypervisor}.

Ein weiterer Hypervisor ist die Kernel-basierte Virtual Machine (\textit{KVM}).
KVM ist in den Linux-Kernel \footnote{\url{https://www.kernel.org}} integriert und wandelt
das Linux-Betriebssystem in einen Typ 1 Hypervisor. Sie unterstützt
Hardware-Virtualisierungserweiterungen, wie beispielsweise Intel-VT
\footnote{\url{https://www.intel.de/content/www/de/de/virtualization/virtualization-technology/intel-virtualization-technology.html}}.
Diese Erweiterungen wurden eingeführt,
um die Virtualisierung und die Entwicklung
von Hypervisor zu beschleunigen \cite{IBMHypervisor} \cite{RedHatKVM}.

\section{Container}
Virtualisierung ist auch mithilfe von Linux Container möglich,
was auch Containerisierung (engl. \textit{Containerization})
genannt wird. Diese enthalten isolierte Prozesse
\footnote{\url{https://www.tldp.org/LDP/tlk/kernel/processes.html}}.
Container teilen sich den Host-Kernel. Durch eine
Konfigurationsdatei (\textit{Image}) werden alle
nötigen Ressourcen und Dateien definiert, die für das Ausführen
der Prozesse benötigt werden. Dadurch spart man sich ein komplettes
Betriebsystem zu laden, wodurch sie den Vorteil,
dass sie kompakter und sparsamer sind. Entwickler steht eine eindeutig
definierte Laufzeitumgebung zur Verfügung, auf der sie ihre Anwendungen testen
können \cite{RedHatContainer}. Container bauen auf zwei Komponenten
des Linux-Kernels auf, den Namensräumen
(\textit{namespaces} \footnote{\url{http://manpages.ubuntu.com/manpages/focal/man7/namespaces.7.html}})
und den Kontrollgruppen
(\textit{cgroups} \footnote{\url{http://manpages.ubuntu.com/manpages/focal/en/man7/cgroups.7.html}}).

Mithilfe von namespaces stellt der Kernel eine Isolationsmöglichkeit
für Prozesse bereit. Mit ihnen können globale Ressourcen abstrahiert werden.
Prozesse innerhalb des namespaces sehen nur ihre isolierte lokale
Ausprägung der Ressource \cite{UbuntuNamespaces}.
Dadurch können mehrere Container gleichzeitig auf die gleiche
Ressource zugreifen \cite{RedHatIntroToLinuxContainers}.
Es existieren unterschiedliche namespaces, wie beispielsweise
\textit{mount namespaces} \footnote{\url{http://manpages.ubuntu.com/manpages/focal/man7/mount_namespaces.7.html}},
\textit{network namespaces} \footnote{\url{http://manpages.ubuntu.com/manpages/focal/man7/network_namespaces.7.html}}
und \textit{pid namespaces} \footnote{\url{http://manpages.ubuntu.com/manpages/focal/man7/pid_namespaces.7.html}}.
Durch mount namespaces kann die Sicht auf das Dateisystem
(engl. \textit{filesystem}) für Prozesse beschränkt werden.
Mit network namespaces lassen sich IP-Adressen, Ports
und Routing Tabellen isolieren. PID (deut. Prozess-ID) namespaces teilen
den Identifikationsbereich. Ein Prozess hat dadurch zwei
PIDs. Eine Identifaktionsnummer auf dem Host-Betriebssystem und eine
innerhalb des namespaces, welche sich beide unterscheiden können
\cite{LwnDotNetNamespaces}.

Cgroups haben zwei wichtige Aufgaben. Sie erlauben das Gruppieren
von Prozessen und können dynamisch Ressourcen für Gruppierungen begrenzen.
Begrenz werden können beispielsweise CPU Zeit und Speicher eines Prozesses.
Möglich wird dies durch \textit{subsystems},
auch \textit{resource controller} genannt. Subsytems sind Bestandteil
des Kernels und erlauben das Ändern von Prozesseigenschaften und
Prozessverhalten in einer cgroups Gruppierung. 
\cite{RedHatIntroToLinuxContainers}.


% -- Ein Cloud Computing Service wie PaaS, IaaS, SaaS\\
% -- Kleine, stateless Funktionen die einzelnd abgerechnet und skaliert werden\\

% \section{Serverless Containers}
% -- Laufzeitumgebung der serverlosen Anwendung wird durch Dockerimage bereitgestellt
% -- Anwendung kann beliebigen aufbau haben

% \section{Open-Source und self-hosted Lösungen}
% -- OpenFaaS
% -- IBM OpenWhisk
% -- Oracle Fn Project

% \section{Sichere Umgebungen}
% -- VMs sind zu Ressourcen intensiv
% -- Container sind nicht sicher

% \subsection{Google gVisor}
% -- Applikationskernel für Container in Golang\\
% -- Implementiert viele des Linux kernel interfaces\\
% -- Fängt SystemCalls ab und gibt sich als GuestKernel aus\\
% % TODO: siehe gVisor Doku und Whitepapers\\

% \subsection{AWS Firecracker}
% -- Benutzt KVM (Kernel-based Virtaul Machine, ähnlich sVMs)\\
% % TODO: https://assets.amazon.science/96/c6/302e527240a3b1f86c86c3e8fc3d/firecracker-lightweight-virtualization-for-serverless-applications.pdf

% \section{Eigenschaften serverloser Architekturen}
% -- Architekturdartstellung einer simplen Webanwendung, Frontend + Backend + Datenbank
% -- Vier eigenschaften des Serverless Computing bei jedem der Services deutlich machen
% -- Serverless sieht vor soviel wie möglich die Dienste der Cloud-Provider zuverwenden